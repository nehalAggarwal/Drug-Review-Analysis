# -*- coding: utf-8 -*-
"""CSI5386_Nehal_Ashish_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w05b5IvssVydp0fC2zXyHdA3ep5MYu70

# CSI5386 - Natural Language Processing | Project

### **Nehal Aggarwal | 300092092** | nagga097@uottawa.ca

### **Ashish Verma | 300059114** | averm019@uottawa.ca 

## **University Of Ottawa**

### Mounting Google Drive
"""

# Load the Drive helper and mount
from google.colab import drive

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd /content/drive/My Drive/Term5/NLP/Project

"""#### Loading all required  paths"""

dataPath = '/content/drive/My Drive/Term5/NLP/Project/Dataset'
outputPath = '/content/drive/My Drive/Term5/NLP/Project/Output'
drug_train = dataPath + '/drugsComTrain_raw.tsv'
drug_test = dataPath + '/drugsComTest_raw.tsv'
glove_vectors_file = dataPath + '/word_embeddings/glove.840B.300d.txt'

"""### Loading Dataset and Pre-processing

#### Helper Methods
"""

import pandas as pd

def readFile(fileName):
  filedata = pd.read_csv(fileName, delimiter='\t', header = None, skiprows=1)
  filedata.columns = ["id", "drugName", "condition", "review", "rating", "date", "usefulCount"]
  return filedata

def saveDataToCSV(df, file_name):
  df.to_csv(dataPath +'/'+file_name, sep='\t', encoding='utf-8', index=False)
  print(file_name+'...saved to csv')

"""#### Loading embeddings"""



"""#### Dataset | Creating validation Set | To be run only once"""

train_data_raw = readFile(drug_train)
test_data_raw = readFile(drug_test)

##This code should be executed only in the initial phase to generate validation set.
from sklearn.model_selection import train_test_split

train_data, valid_data = train_test_split(train_data_raw, test_size=0.2)

#saveDataToCSV(train_data, 'drugsComTrain.csv')
#saveDataToCSV(valid_data, 'drugsComValid.csv')
#saveDataToCSV(test_data_raw, 'drugsComTest.csv')

"""#### Dataset | Loading"""

drug_train = dataPath + '/drugsComTrain.csv'
drug_test = dataPath + '/drugsComTest.csv'
drug_valid = dataPath + '/drugsComValid.csv'

train_data = readFile(drug_train)
valid_data = readFile(drug_valid)
test_data = readFile(drug_test)

print("Training set size {}".format(train_data.size))
print("Validation set size {}".format(valid_data.size))
print("Testing set size {}".format(test_data.size))

train_data.head(3)

"""#### Dataset | Analysing

Helper methods
"""

import matplotlib.pyplot as plt
def drugsPerConditionGraph(data, groupby='', col='', asc=False): 
  top_drugCondition = data.groupby([groupby])[col].nunique().sort_values(ascending=asc)
  top_drugCondition[0:30].plot(kind="bar", figsize = (14,6), fontsize = 10,color="grey")
  plt.xlabel("Condition", fontsize = 20)
  plt.ylabel("No. of drugs", fontsize = 20)
  plt.title("The number of drugs per condition.", fontsize = 20)
  plt.figure()

def findMissing(data):
  missing = (data.isnull().sum()).sort_values(ascending=False)
  print("Missing value (%):", missing['condition']/data.shape[0] *100)
  return missing

def wrongConditions(data):
  x = data[data['condition']=='3</span> users found this comment helpful.']
  print(x)

print("Unique values of ids in Training set : " ,len(train_data['id'].values))
print("Instances in Training set: " ,train_data.shape[0])

train_data["strlen"] = train_data['review'].str.split().str.len()
valid_data["strlen"] = valid_data['review'].str.split().str.len()
test_data["strlen"] = test_data['review'].str.split().str.len()

import matplotlib.pyplot as plt
def reviewLength(data):
  fig = plt.figure(figsize=(10, 10)) 
  plt.xlabel('Sentence length')
  plt.ylabel('Number of sentences')
  plt.title('Length of Sentences')
  plt.hist(data["strlen"], bins=86)
  plt.show()

  maxLen = data["strlen"].max()
  minLen = data["strlen"].min()
  meanLen = data["strlen"].mean()

  print ( " Maximum review length %s \n Minimum review length %s \n Mean review length %s \n" %(maxLen, minLen, meanLen))

  a,b,c,d,e = 0,0,0,0,0
  for x in data['strlen']:
    if x >=1000:
      a = a+1
    if x >=750:
      b = b+1
    if x >=250:
      c = c+1
    if x >=100:
      d=d+1
    if x <100:
      e = e+1

  print (" No. of Reviews by Length \n %s >1000 words \n %s >750 words \n %s >250 words \n %s >100 words \n %s <100 words\n" % (a,b,c,d,e))

for d in [train_data, valid_data, test_data]:
  drugsPerConditionGraph(d, 'condition', 'drugName')
  missingVal = findMissing(d)
  print(missingVal)
  wrongConditions(d)
  reviewLength(d)

"""#### Dataset | Cleaning"""

def removeSpanFromCondition(data):
  span_list = []
  for i,j in enumerate(data['condition']):
      if '</span>' in j:
          span_list.append(i)
  return data.drop(data.index[span_list], inplace = True)

def removeOneDrugCondition(data):
  oneDrug_list = []
  oneDrugCond = data.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)
  oneDrugCond = pd.DataFrame(oneDrugCond).reset_index()
  condition_1 = oneDrugCond[oneDrugCond['drugName']==1]
  for i,j in enumerate(data['condition']):
      for c in list(condition_1['condition']):
          if j == c:
              oneDrug_list.append(i)
  return data.drop(data.index[oneDrug_list], inplace = True)

import re
from nltk.tokenize import RegexpTokenizer
def removePunctuationMarks(text):
  tokenizer = RegexpTokenizer(r'\w+')
  result = tokenizer.tokenize(text)
  nopuncttext = " ".join(result)
  return nopuncttext

def removeNonEnglishChars(text):
  text = re.sub(r'[^\x00-\x7f]',r'', text)
  return text

def removeUnderscore(text):
  text = text.replace("_", "");
  return text

def replaceApostrophe(text):
  text = text.replace("&#039;","\'")
  return text

def removeThings(text):
  t = replaceApostrophe(text)
  t = removePunctuationMarks(t)
  t = removeNonEnglishChars(t)
  t = removeUnderscore(t)
  return t

for d in [train_data, valid_data, test_data]:
  d.dropna(axis=0, inplace = True)
  removeSpanFromCondition(d)
  removeOneDrugCondition(d)

train_data.head(3)

train_data['clean_review'] = train_data['review'].apply(removeThings)
valid_data['clean_review'] = valid_data['review'].apply(removeThings)
test_data['clean_review'] = test_data['review'].apply(removeThings)

pd.set_option('display.max_colwidth', -1)

train_data.head(3)

train_data['clean_review']
valid_data['clean_review']
test_data['clean_review']

train_data = train_data.loc[train_data["strlen"] <= 200]
valid_data = valid_data.loc[valid_data["strlen"] <= 200]
test_data = test_data.loc[test_data["strlen"] <= 200]

"""#### Dataset | Analyses Post cleaning"""

for d in [train_data, valid_data, test_data]:
  drugsPerConditionGraph(d, 'condition', 'drugName')
  missingVal = findMissing(d)
  print(missingVal)
  wrongConditions(d)
  reviewLength(d)

train_data.drop(['strlen'], axis=1, inplace=True)
valid_data.drop(['strlen'], axis=1, inplace=True)
test_data.drop(['strlen'], axis=1, inplace=True)

"""#### Dataset | Pre-Processing"""

all_data = pd.concat([train_data, valid_data, test_data])
pos_all_data = all_data[all_data["rating"]>5]
neg_all_data = all_data[all_data["rating"]<6]

from plotly import subplots
from collections import defaultdict
from nltk.corpus import stopwords 
import plotly.graph_objs as go
import plotly.offline as py

import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def generateNgrams(text, n=1):
    token = [token for token in text.lower().split(" ") if token != "" if token not in stop_words]
    ngrams = zip(*[token[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]

def drawNgramChart(data, color, n):
    freqDict = defaultdict(int)
    for line in data["review"]:
        for word in generateNgrams(line, n):
            freqDict[word] += 1
            
    sortedFreqDict = pd.DataFrame(sorted(freqDict.items(), key=lambda x: x[1])[::-1])
    sortedFreqDict.columns = ["word", "wordcount"]
    sfd = sortedFreqDict.head(50)

    trace = go.Bar(
        y=sfd["word"].values[::-1],
        x=sfd["wordcount"].values[::-1],
        showlegend=False,
        orientation = 'h',
        marker=dict(
            color=color,
        ),
    )
    return trace

def createSubplots(tr0, tr1, n):
  fig = subplots.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.15,
                            subplot_titles=["Frequent " + str(n) +"-grams of rating 1 to 5", 
                                            "Frequent " + str(n) + "-grams of rating 6 to 10"])
  fig.append_trace(tr0, 1, 1)
  fig.append_trace(tr1, 1, 2)
  fig['layout'].update(height=1200, width=1600, paper_bgcolor='rgb(233,233,233)', title= str(n) + "-gram Count Plots")
  py.iplot(fig, filename='word-plots')

trace0_4 = drawNgramChart(neg_all_data, 'grey',4)
trace1_4 = drawNgramChart(pos_all_data, 'grey',4)

trace0_5 = drawNgramChart(neg_all_data, 'grey',5)
trace1_5 = drawNgramChart(pos_all_data, 'grey',5)

createSubplots(trace0_4, trace1_4, 4)
createSubplots(trace0_5, trace1_5, 5)

"""Instance Review Preprocessing (Cleaning)"""

from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
nltk.download('stopwords')
import re

not_stop = ["aren't","couldn't","didn't","doesn't","don't","hadn't","hasn't","haven't","isn't","mightn't","mustn't","needn't","no","nor","not","shan't","shouldn't","wasn't","weren't","wouldn't"]
for i in not_stop:
    stop_words.remove(i)

stemmer = SnowballStemmer('english')

def stemmetizeSentence(review):
    text = re.sub('[^a-zA-Z]', ' ', review)
    words = text.lower().split()
    meaningWords = [w for w in words if not w in stop_words]
    stemmedWords = [stemmer.stem(w) for w in meaningWords]
    return( ' '.join(stemmedWords))

train_data['clean_review'] = train_data['clean_review'].apply(stemmetizeSentence)
valid_data['clean_review'] = valid_data['clean_review'].apply(stemmetizeSentence)
test_data['clean_review'] = test_data['clean_review'].apply(stemmetizeSentence)

train_data.head(3)
 x = train_data['clean_review'].head(5)
 print(x)

"""Model"""

train_data['sentiment'] = train_data["rating"].apply(lambda x: 1 if x > 5 else 0)
valid_data['sentiment'] = valid_data["rating"].apply(lambda x: 1 if x > 5 else 0)
test_data['sentiment'] = test_data["rating"].apply(lambda x: 1 if x > 5 else 0)

saveDataToCSV(train_data, 'cleaned_Train_data.csv')
saveDataToCSV(valid_data, 'cleaned_Valid_data.csv')
saveDataToCSV(test_data, 'cleaned_Test_data.csv')

"""For Text Polarity"""

drug_train = dataPath + '/cleaned_Train_data.csv'
drug_test = dataPath + '/cleaned_Test_data.csv'
drug_valid = dataPath + '/cleaned_Valid_data.csv'

def readFile1(fileName):
  filedata = pd.read_csv(fileName, delimiter='\t', header = None, skiprows=1)
  filedata.columns = ["id", "drugName", "condition", "review", "rating", "date", "usefulCount", "clean_review", "sentiment"]
  #filedata.drop(['review'], axis=1, inplace = True)
  filedata.dropna(axis=0, inplace=True)
  return filedata

train_data = readFile1(drug_train)
valid_data = readFile1(drug_valid)
test_data = readFile1(drug_test)

from textblob import TextBlob
def predictSentiment(data):
  blob = TextBlob(data)
  Predict_Sentiment = blob.sentiment.polarity
  return Predict_Sentiment

train_data["Predict_Sentiment"] = train_data["review"].apply(predictSentiment)
valid_data["Predict_Sentiment"] = valid_data["review"].apply(predictSentiment)
test_data["Predict_Sentiment"] = test_data["review"].apply(predictSentiment)

train_data.head(20)