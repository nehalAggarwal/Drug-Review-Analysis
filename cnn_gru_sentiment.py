# -*- coding: utf-8 -*-
"""CNN_GRU_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTr4RMlG4DtL58aIurRR8VF9YF4OuNOG
"""

# Load the Drive helper and mount
from google.colab import drive

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd /content/drive/My Drive/Term5/NLP/Project

import pandas as pd
from keras.preprocessing.text import Tokenizer

def readFile(fileName):
  filedata = pd.read_csv(fileName, delimiter='\t', header = None, skiprows=1)
  filedata.columns = ["id", "drugName", "condition", "review", "rating", "date", "usefulCount", "clean_review", "sentiment"]
  filedata.drop(['review'], axis=1, inplace = True)
  filedata.dropna(axis=0, inplace=True)
  return filedata

dataPath = '/content/drive/My Drive/Term5/NLP/Project/Dataset'
outputPath = '/content/drive/My Drive/Term5/NLP/Project/Model_output/CNN_GRU/'
glove_vectors_file = dataPath + '/../word_embeddings/glove.840B.300d.txt'

drug_train = dataPath + '/cleaned_Train_data.csv'
drug_test = dataPath + '/cleaned_Test_data.csv'
drug_valid = dataPath + '/cleaned_Valid_data.csv'

train_data = readFile(drug_train)
valid_data = readFile(drug_valid)
test_data = readFile(drug_test)

train_data.head(3)

import io
import numpy as np

#http://nlp.stanford.edu/data/glove.840B.300d.zip
glove_embeddings = {}

print('Loading '+ glove_vectors_file)
with open(glove_vectors_file, 'r') as f:
  for line in f:
    values = line.split(' ')
    word = values[0]
    embedding = np.asarray(values[1:], dtype='float32')
    glove_embeddings[word] = embedding
        
print('Embeddings size: %d' % len(glove_embeddings))

embedding_dim = 300

def convertSentencesToList(traindata, validdata):
  train_sentence = traindata['clean_review'].values.tolist()
  valid_sentence = validdata['clean_review'].values.tolist()
  return train_sentence, valid_sentence

def converToWordSequences(embeddinglength, training_data, t_s, v_s):
  tokenizer = Tokenizer(num_words = embeddinglength)
  tokenizer.fit_on_texts(training_data)

  t_s_word_sequences = tokenizer.texts_to_sequences(t_s)

  v_s_word_sequences = tokenizer.texts_to_sequences(v_s)
  return tokenizer, t_s_word_sequences, v_s_word_sequences

train_sent, valid_sent = convertSentencesToList(train_data, valid_data)

glove_length = len(glove_embeddings)

print('Found '+ str(len(train_sent)) + ' in training sentence')

training_sent = train_sent
valid_sent = valid_sent

tokenizer_g, t_s_word_sequences_g, v_s_word_sequences_g = converToWordSequences(glove_length, training_sent, train_sent, valid_sent)
word_index_g = tokenizer_g.word_index

print('Found '+str(len(word_index_g))+' unique tokens for glove sequencing.')

from collections import defaultdict

def useWordEmbeddings(word_index, NUM_WORDS, embeddings, sentence_word_sequences):
  words_len = min(NUM_WORDS, len(word_index))
  # word_embedding_matrix = np.zeros((words_len + 1, embedding_dim))
  word_embedding_matrix = np.random.random((words_len + 1, embedding_dim))
  k = 0
  for word, i in word_index.items():
    if i >= NUM_WORDS:
      continue
    embedding_vector = embeddings.get(word)
    if embedding_vector is not None:
      word_embedding_matrix[i] = embedding_vector
      k += 1

  return word_embedding_matrix

word_embedding_matrix_g = useWordEmbeddings(word_index_g, glove_length, glove_embeddings, t_s_word_sequences_g)

max_seq_length = 300

from keras.preprocessing.sequence import pad_sequences

def convertToPadSequences(sentence_word_sequences, valid_sentence_word_sequences, max_seq_length):
  s1_data = pad_sequences(sentence_word_sequences, maxlen = max_seq_length)
  s1_dataValid = pad_sequences(valid_sentence_word_sequences, maxlen = max_seq_length)
  return s1_data, s1_dataValid

def printSentenceInfo(s1_data, labels):
  print('Shape of sentence1 tensor:', s1_data.shape)
  print('Shape of label tensor:', labels.shape)

s1_data_g, s1_dataValid_g = convertToPadSequences(t_s_word_sequences_g, v_s_word_sequences_g, max_seq_length)

from keras.utils import np_utils, plot_model
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

def convertToCategorical(data):
  labels = np_utils.to_categorical(le.fit_transform(data["sentiment"].values)).astype("int64")
  return labels

train_labels = convertToCategorical(train_data)
valid_labels = convertToCategorical(valid_data)

printSentenceInfo(s1_data_g, train_labels)

from keras.layers import LSTM, Embedding, concatenate, Dense, PReLU, Dropout, Activation, Conv1D, MaxPooling1D, Flatten, SimpleRNN, GRU
from keras.models import Sequential
from keras.initializers import Constant
from keras.layers.normalization import BatchNormalization
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from IPython.display import Image
import time
import matplotlib.pyplot as plt

num_rnn = np.random.randint(175, 275)
num_dense = np.random.randint(100, 150)
rate_drop_rnn = 0.15 + np.random.rand() * 0.25
rate_drop_dense = 0.15 + np.random.rand() * 0.25

batch_size = 128

def trainModel(model, model_name, s1_data, labels, batch_size, s1_data_valid, labels_valid):
  checkpoint = ModelCheckpoint(outputPath+ model_name + '-checkpoint-weights.{epoch:02d}-{val_accuracy:.2f}.hdf5', monitor='val_accuracy', save_best_only=True)
  history = model.fit([s1_data],
                        labels,
                        epochs = 10,
                        batch_size = batch_size,
                        validation_data=([s1_data_valid], labels_valid),
                        shuffle = False,
                        verbose = 2,
                        callbacks = [checkpoint])
  return history

def plotModelPicture(model, model_name):
  plot_model(model, to_file = outputPath + model_name + '-model.png', show_shapes=False)

def plotMetricsGraph(modelhistory, model_name):
  # summarize history for accuracy
  plt.plot(modelhistory.history['accuracy'])
  plt.plot(modelhistory.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'valid'], loc='upper left')
  plt.show()
  plt.savefig(model_name + '-acc.png', dpi = 300)

  # summarize history for loss
  plt.plot(modelhistory.history['loss'])
  plt.plot(modelhistory.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'valid'], loc='upper left')
  plt.show()
  plt.savefig(model_name + '-loss.png', dpi = 300)

def evaluateModel(model, test_s_data, test_labels):
  loss, accuracy = model.evaluate([test_s_data], test_labels)
  print('loss     = ', loss)
  print('accuracy = ', accuracy)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plotConfusionMatrix(test_y, y_pred):
  #y_pred=model.predict(test_x,batch_size=15)
  #cm =confusion_matrix(test_y.argmax(axis=1), y_pred.argmax(axis=1))
  cm =confusion_matrix(test_y, y_pred)
  print(cm)
  index = ['Negative','Positive']
  columns = ['Negative','Positive']
  cm_df = pd.DataFrame(cm,columns,index)             
  plt.figure(figsize=(10,6))
  sns.heatmap(cm_df, annot=True)

def CNNGRUModel(word_index, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, p_drop=0.0):
  encoder_1 = Sequential()
  encoder_1.add(Embedding(len(word_index) + 1, EMBEDDING_DIM,
                                    embeddings_initializer=Constant(embedding_matrix),
                                    input_length=MAX_SEQUENCE_LENGTH,
                                    trainable=False))

  encoder_1.add(Conv1D(128, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding="same"))
  encoder_1.add(Conv1D(64, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(64, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(32, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(32, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(16, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(16, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(8, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(Conv1D(8, kernel_size = 2, padding = "same"))
  encoder_1.add(MaxPooling1D(pool_size=2, padding='same'))
  encoder_1.add(GRU(128,return_sequences=True))
  encoder_1.add(Dropout(0.3))
  encoder_1.add(Flatten())
  encoder_1.add(Dense(128,activation='relu'))
  encoder_1.add(Dropout(0.5))
  encoder_1.add(Dense(2, activation="sigmoid", name="final_output"))

  encoder_1.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=0.001), metrics = ['accuracy'])
    
  return encoder_1

model_cnn_gru_g = CNNGRUModel(word_index_g, embedding_dim, word_embedding_matrix_g, max_seq_length, p_drop=0.25)
model_cnn_gru_g.summary()

model_name = 'CNN_GRU_Glove'
plotModelPicture(model_cnn_gru_g, model_name)
Image(filename = outputPath + model_name + '-model.png')

history_cnn_gru_g = trainModel(model_cnn_gru_g, model_name, s1_data_g, train_labels, batch_size, s1_dataValid_g, valid_labels)

plotMetricsGraph(history_cnn_gru_g, model_name)

def getTestDataSequences(tokenizer):
  test_s_word_sequences = tokenizer.texts_to_sequences(test_data['clean_review'].values.tolist())

  test_s_data = pad_sequences(test_s_word_sequences, maxlen = max_seq_length)
  test_labels = np_utils.to_categorical(le.fit_transform(test_data["sentiment"].values)).astype("int64")

  print('Shape of test sentence1 tensor:', test_s_data.shape)
  print('Shape of test label tensor:', test_labels.shape)
  return test_s_data, test_labels

test_s_data_g, test_labels_g = getTestDataSequences(tokenizer_g)

evaluateModel(model_cnn_gru_g, test_s_data_g, test_labels_g)

model = model_cnn_gru_g

test_pred = model.predict([test_s_data_g], batch_size=32)

output = list()
output_score = list()
for x in test_pred:
  #print(type(x))
  output_score.append(np.amax(x))
  output.append(np.argmax(x))

test_data["Actual Output"] = output
test_data["Predicted Score"] = output_score

test_data.to_excel(outputPath+'output_cnn_gru_glove.xlsx')

from sklearn.metrics import classification_report

predicted = np.argmax(test_pred, axis=1)
report = classification_report(test_data["sentiment"], test_data["Actual Output"])
print(report)

plotConfusionMatrix(test_data["sentiment"], test_data["Actual Output"])